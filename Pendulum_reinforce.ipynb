{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " \"Pendulum_reinforce.ipynb\"",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ungHL5bCKPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL_AquFmqxqk",
        "colab_type": "text"
      },
      "source": [
        "Neural Network as a policy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKwaYelZa6v5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import torch  \n",
        "import gym\n",
        "import numpy as np  \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Constants\n",
        "GAMMA = 0.9999\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, num_inputs, num_actions, hidden_size, learning_rate=3e-6):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "\n",
        "        self.num_actions = num_actions\n",
        "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
        "        self.linear3 = nn.Linear(hidden_size, hidden_size*2)\n",
        "        self.linear4 = nn.Linear(hidden_size*2, hidden_size*4)\n",
        "        self.linear5 = nn.Linear(hidden_size*4, hidden_size*2)\n",
        "        self.linear2 = nn.Linear(hidden_size*2, num_actions)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.tanh(self.linear5(F.tanh(self.linear4(F.tanh(self.linear3(F.tanh(self.linear1(state))))))))\n",
        "        x = F.tanh(self.linear2(x))\n",
        "        return x \n",
        "    \n",
        "    def get_action(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        probs = self.forward(Variable(state))\n",
        "        #highest_prob_action = np.random.choice(self.num_actions, p=np.squeeze(probs.detach().numpy()))\n",
        "        #log_prob = torch.log(probs.squeeze(0)[highest_prob_action])\n",
        "        rands = np.random.normal(size = probs.shape[0])\n",
        "        highest_prob_action = torch.tensor(rands, dtype = torch.float32) * torch.sqrt(probs[:,1]**2) +  probs[:,0]\n",
        "        log_prob = torch.log(torch.exp(-((highest_prob_action-probs[:,0])**2)/(2*probs[:,1]**2))*torch.sqrt(2*np.pi*probs[:,1]**2))\n",
        "        return highest_prob_action, log_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiMQjSx7T_AH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_policy(policy_network, rewards, log_probs):\n",
        "    discounted_rewards = []\n",
        "    l = []\n",
        "    for t in range(len(rewards)):\n",
        "        Gt = 0 \n",
        "        pw = 0\n",
        "        for r in rewards[t:]:\n",
        "            Gt = Gt + GAMMA**pw * r\n",
        "            pw = pw + 1\n",
        "        discounted_rewards.append(Gt)\n",
        "        \n",
        "    discounted_rewards = torch.tensor(discounted_rewards)\n",
        "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9) # normalize discounted rewards\n",
        "\n",
        "    policy_gradient = []\n",
        "    for log_prob, Gt in zip(log_probs, discounted_rewards):\n",
        "      policy_gradient.append(-log_prob * Gt)\n",
        "    \n",
        "    policy_network.optimizer.zero_grad()\n",
        "    policy_gradient = torch.stack(policy_gradient).sum()\n",
        "    policy_gradient.backward()\n",
        "    policy_network.optimizer.step()\n",
        "    l.append(policy_gradient)\n",
        "    return l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lCR-KqLLyL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"MountainCarContinuous-v0\")\n",
        "policy_net = PolicyNetwork(env.observation_space.shape[0], 2, 128)\n",
        "    \n",
        "max_episode_num = 5000\n",
        "max_steps = 10000\n",
        "numsteps = []\n",
        "avg_numsteps = []\n",
        "all_rewards = []\n",
        "l = []\n",
        "for episode in range(max_episode_num):\n",
        "  state = env.reset()\n",
        "  log_probs = []\n",
        "  rewards = []\n",
        "\n",
        "  for steps in range(max_steps):\n",
        "    #plt.imshow(env.render(\"rgb_array\"))\n",
        "    action, log_prob = policy_net.get_action(state)\n",
        "  \n",
        "    new_state, reward, done, _ = env.step(action.detach().numpy())\n",
        "    log_probs.append(log_prob)\n",
        "    rewards.append(reward)\n",
        "\n",
        "    if steps == max_steps-1 :\n",
        "      loss = update_policy(policy_net, rewards, log_probs)\n",
        "      l.append(loss)\n",
        "      numsteps.append(steps)\n",
        "      avg_numsteps.append(np.mean(numsteps[-10:]))\n",
        "      all_rewards.append(np.sum(rewards))\n",
        "      if episode % 1 == 0:\n",
        "        sys.stdout.write(\"episode: {}, total reward: {}, average_reward: {}, length: {}\\n\".format(episode, np.round(np.sum(rewards), decimals = 3),  np.round(np.mean(all_rewards[-10:]), decimals = 3), steps))\n",
        "        print(loss)\n",
        "        break\n",
        "            \n",
        "    state = new_state\n",
        "        \n",
        "plt.plot(numsteps)\n",
        "plt.plot(avg_numsteps)\n",
        "plt.xlabel('Episode')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgGMpIP9Scvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf8mMWr1BDZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}